{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I am going to implement a Deep Neural Network using TensorFlow 2.0 in a dataset called MNIST, which is about handwritten digit recognition. This is a classification problem.\n",
    "\n",
    "The dataset I am going to use is the one provided by TensorFlow and consist of 70,000 images with 28x28 pixels of handwritten numbers. That means, each image corresponds to a number between 0 and 9.\n",
    "\n",
    "My goal is to write an algorithm that identifies the handwritten number of the given images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# the dataset is download the first time to my local \n",
    "# machine and the next time the code is run, the computer\n",
    "# will just load the data from the stored folder\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST dataset for tensorflow dataset\n",
    "# with_info=True to know version, size, etc. of the data \n",
    "# as_supervised=True return 2-tuple, input and target, structure\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# we define the train and test set which are from default \n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no validation set, so we take a 10% from train set\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "# just in case the number of samples is not an integer\n",
    "# we transform it to an integer\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we create a variable with the number of test\n",
    "# samples and we tranform from float to an integer\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is normalised to have values between 0 and 1, instead of between 0 and 255. This is a common practice in ML, since it is desirable to have the same range of values for all the variables. This is done dividing by the maximum with a function and applying it to all the pixels in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is a grey scale from 0 to 255\n",
    "# I define a function called scale to \n",
    "# translate it into a numbers from 0 to 1\n",
    "def scale(image, label):\n",
    "    # the value has to be a float to scale it\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# application of the function for dataset\n",
    "# first from the train that will be split \n",
    "# into train and validation\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "# scale function applied to test dataset\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, a common practice in ML is to shuffle the dataset. This is very useful practice to avoid local minimum for the loss function and to avoid ordered data that could confuse the neural network to make a wrong assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with big datasets can make the computer run\n",
    "# out of memory, therefore it is define a buffer_size \n",
    "# to shuffle by smaller groups of data\n",
    "# 1 (no shuffle) < buffer_size <= num samples \n",
    "buffer_size = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to extract the validation data from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation data is taken from the train dataset\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "# the train data is the rest \n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
