{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I am going to implement a Deep Neural Network using TensorFlow 2.0 in a dataset called MNIST, which is about handwritten digit recognition. This is a classification problem.\n",
    "\n",
    "The dataset I am going to use is the one provided by TensorFlow and consist of 70,000 images with 28x28 pixels of handwritten numbers. That means, each image corresponds to a number between 0 and 9.\n",
    "\n",
    "My goal is to write an algorithm that identifies the handwritten number of the given images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# the dataset is download the first time to my local \n",
    "# machine and the next time the code is run, the computer\n",
    "# will just load the data from the stored folder\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST dataset for tensorflow dataset\n",
    "# with_info=True to know version, size, etc. of the data \n",
    "# as_supervised=True return 2-tuple, input and target, structure\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# we define the train and test set which are from default \n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no validation set, so we take a 10% from train set\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "# just in case the number of samples is not an integer\n",
    "# we transform it to an integer\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we create a variable with the number of test\n",
    "# samples and we tranform from float to an integer\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is normalised to have values between 0 and 1, instead of between 0 and 255. This is a common practice in ML, since it is desirable to have the same range of values for all the variables. This is done dividing by the maximum with a function and applying it to all the pixels in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is a grey scale from 0 to 255\n",
    "# I define a function called scale to \n",
    "# translate it into a numbers from 0 to 1\n",
    "def scale(image, label):\n",
    "    # the value has to be a float to scale it\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# application of the function for dataset\n",
    "# first from the train that will be split \n",
    "# into train and validation\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "# scale function applied to test dataset\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, a common practice in ML is to shuffle the dataset. This is very useful practice to avoid local minimum for the loss function and to avoid ordered data that could confuse the neural network to make a wrong assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with big datasets can make the computer run\n",
    "# out of memory, therefore it is define a buffer_size \n",
    "# to shuffle by smaller groups of data\n",
    "# 1 (no shuffle) < buffer_size <= num samples \n",
    "buffer_size = 1000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to extract the validation data from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation data is taken from the train dataset\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "# the train data is the rest \n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# the train data is also batched\n",
    "# to be able to iterate over diff batches\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# as_supervized=True means there are two tuples \n",
    "# for each data (input and targets)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already clean and prepare to be used, therefore now is time to work on the model (deep learning algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28 # 28 times 28 pixels are 784 total inputs\n",
    "output_size = 10 # ten possible digits from 0 till 9\n",
    "\n",
    "hidden_layer_size = 500 # also called depth\n",
    "\n",
    "# the model used is sequential\n",
    "model = tf.keras.Sequential([\n",
    "    # the first layer is the input layer, it is defined as a\n",
    "    # tensor 28x28x1 that with the 'Flatten' is translated to \n",
    "    # a vector\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)), # input layer\n",
    "    \n",
    "    # the hidden layer needs to choose the activation function \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 5th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 6th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 7th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 8th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 9th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 10th hidden layer\n",
    "    \n",
    "    #the last layer is the output, the activation function is with 'softmax'\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to define the optimizer, the loss function and the metric that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is done inside the 'compile' options\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "540/540 - 25s - loss: 0.3215 - accuracy: 0.9065 - val_loss: 0.1882 - val_accuracy: 0.9530\n",
      "Epoch 2/8\n",
      "540/540 - 28s - loss: 0.1458 - accuracy: 0.9641 - val_loss: 0.1535 - val_accuracy: 0.9635\n",
      "Epoch 3/8\n",
      "540/540 - 25s - loss: 0.1116 - accuracy: 0.9721 - val_loss: 0.1483 - val_accuracy: 0.9687\n",
      "Epoch 4/8\n",
      "540/540 - 25s - loss: 0.0943 - accuracy: 0.9773 - val_loss: 0.1593 - val_accuracy: 0.9635\n",
      "Epoch 5/8\n",
      "540/540 - 26s - loss: 0.0850 - accuracy: 0.9799 - val_loss: 0.1336 - val_accuracy: 0.9703\n",
      "Epoch 6/8\n",
      "540/540 - 27s - loss: 0.0695 - accuracy: 0.9834 - val_loss: 0.1561 - val_accuracy: 0.9695\n",
      "Epoch 7/8\n",
      "540/540 - 33s - loss: 0.0627 - accuracy: 0.9854 - val_loss: 0.1194 - val_accuracy: 0.9738\n",
      "Epoch 8/8\n",
      "540/540 - 23s - loss: 0.0673 - accuracy: 0.9847 - val_loss: 0.1240 - val_accuracy: 0.9730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24364014688>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of epochs\n",
    "num_epochs = 8\n",
    "\n",
    "# it is choose to fit the model the train and validation data cleaned above\n",
    "model.fit(train_data, epochs=num_epochs, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to play around the hyperparameters to find the better fit to the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has to be tested with data that has never seen the model before to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1102 - accuracy: 0.9753\n",
      "Test loss: 0.11. Test accuracy: 97.53%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
